{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"name":"StandardTestingRun_Sentiment_Hyper.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"r90PIB5b7XEi"},"outputs":[],"source":["!pip install transformers\n","!pip install seqeval\n","!pip install tensorboardx\n","!pip install torchvision==0.4.0 -f https://download.pytorch.org/whl/cu100/torch_stable.html --no-cache-dir\n","!rm -rf .git\n","!rm -rf apex\n","!git clone https://github.com/NVIDIA/apex\n","!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n","!pip3 install simpletransformers\n","!pip install Afinn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"colab_type":"code","id":"90TsvpNc0GY6","outputId":"a1faab2f-b91b-4cf7-e704-7741bfd9e4c9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"nf7qSGtt7pw-"},"outputs":[],"source":["cd /content/drive/My\\ Drive/Colab\\ Notebooks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"OKs0E_mczgjr"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import gc\n","import csv\n","\n","from CsvReader import ReadCsv\n","from HelperFunctions import ShuffleData, replace_all, ReplaceSentimentsWithIndexes, SplitArticlesIntoSentiments, getMetrics, TransformDataIntoDataframe, CalculateWeights\n","from TrainEvalModel import TrainModelForMultiClass, EvalFromModel\n","\n","data, categories, tendencies = ReadCsv('/content/drive/My Drive/Colab Notebooks Pascal/bert-etiki/etiki-data','test.csv','companies.csv', 'categories.csv','references.csv','tendencies.csv', 'topics.csv')\n","sentimentData = ReplaceSentimentsWithIndexes(data[:,[0,7,13]])\n","labels = np.unique(sentimentData[:,1])\n","weights = CalculateWeights(labels,sentimentData)\n","\n","for i in range(1,6):\n","  for j in range(2,6):\n","    positiveArticles, negativeArticles, controversialArticles = SplitArticlesIntoSentiments(sentimentData)\n","    positiveArticles = ShuffleData(positiveArticles)\n","    negativeArticles = ShuffleData(negativeArticles)\n","    controversialArticles = ShuffleData(controversialArticles)\n","\n","    trainData = np.vstack((positiveArticles[:int(len(positiveArticles)*0.7)],controversialArticles[:int(len(controversialArticles)*0.7)],negativeArticles[:int(len(negativeArticles)*0.7)]))\n","    testData = np.vstack((positiveArticles[int(len(positiveArticles)*0.7):],controversialArticles[int(len(controversialArticles)*0.7):],negativeArticles[int(len(negativeArticles)*0.7):]))\n","    train_df = TransformDataIntoDataframe(trainData)\n","    eval_df = TransformDataIntoDataframe(testData)\n","#------------------------------XLNet------------------------------\n","    algo = 'xlnet'\n","    learn_rate = i*1e-5        \n","    args = {'reprocess_input_data': True,\n","              'overwrite_output_dir': True,\n","              'num_train_epochs':j ,\n","              'silent':True,\n","              \"learning_rate\": learn_rate,\n","              }\n","\n","    model = TrainModelForMultiClass(algo, 'xlnet-base-cased',train_df,3,args)\n","    result, model_outputs, wrong_predictions = EvalFromModel(model,eval_df)\n","\n","    with open('results/learning rate/'+str(j)+'epoch/'+algo+'-'+learn_rate+'-mcc.csv', 'a', newline='') as f:\n","      writer = csv.writer(f)\n","      writer.writerow([result['mcc'],result[\"acc\"],result[\"eval_loss\"]])\n","\n","    getMetrics(result[\"matr\"],3,'learning rate/'+str(j)+'epoch/'+algo+'-'+learn_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(1,5):\n","  for j in range(2,6):\n","    positiveArticles, negativeArticles, controversialArticles = SplitArticlesIntoSentiments(sentimentData)\n","    positiveArticles = ShuffleData(positiveArticles)\n","    negativeArticles = ShuffleData(negativeArticles)\n","    controversialArticles = ShuffleData(controversialArticles)\n","\n","    trainData = np.vstack((positiveArticles[:int(len(positiveArticles)*0.7)],controversialArticles[:int(len(controversialArticles)*0.7)],negativeArticles[:int(len(negativeArticles)*0.7)]))\n","    testData = np.vstack((positiveArticles[int(len(positiveArticles)*0.7):],controversialArticles[int(len(controversialArticles)*0.7):],negativeArticles[int(len(negativeArticles)*0.7):]))\n","    train_df = TransformDataIntoDataframe(trainData)\n","    eval_df = TransformDataIntoDataframe(testData)\n","#------------------------------XLNet------------------------------\n","    algo = 'xlnet'        \n","    learn_rate = i*1e-4        \n","    args = {'reprocess_input_data': True,\n","              'overwrite_output_dir': True,\n","              'num_train_epochs':j ,\n","              'silent':True,\n","              \"learning_rate\": learn_rate,\n","              }\n","\n","    model = TrainModelForMultiClass(algo, 'xlnet-base-cased',train_df,3,args)\n","    result, model_outputs, wrong_predictions = EvalFromModel(model,eval_df)\n","\n","    with open('results/learning rate/'+str(j)+'epoch/'+algo+'-mcc.csv', 'a', newline='') as f:\n","      writer = csv.writer(f)\n","      writer.writerow([result['mcc'],result[\"acc\"],result[\"eval_loss\"]])\n","\n","    getMetrics(result[\"matr\"],3,'learning rate/'+str(j)+'epoch/'+algo+'-'+learn_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(1,3):\n","  for j in range(2,6):\n","    positiveArticles, negativeArticles, controversialArticles = SplitArticlesIntoSentiments(sentimentData)\n","    positiveArticles = ShuffleData(positiveArticles)\n","    negativeArticles = ShuffleData(negativeArticles)\n","    controversialArticles = ShuffleData(controversialArticles)\n","\n","    trainData = np.vstack((positiveArticles[:int(len(positiveArticles)*0.7)],controversialArticles[:int(len(controversialArticles)*0.7)],negativeArticles[:int(len(negativeArticles)*0.7)]))\n","    testData = np.vstack((positiveArticles[int(len(positiveArticles)*0.7):],controversialArticles[int(len(controversialArticles)*0.7):],negativeArticles[int(len(negativeArticles)*0.7):]))\n","    train_df = TransformDataIntoDataframe(trainData)\n","    eval_df = TransformDataIntoDataframe(testData)\n","#------------------------------XLNet------------------------------\n","    algo = 'xlnet'\n","    learn_rate = (10^i)*1e-4       \n","    args = {'reprocess_input_data': True,\n","              'overwrite_output_dir': True,\n","              'num_train_epochs':j ,\n","              'silent':True,\n","              \"learning_rate\": learn_rate,\n","              }        \n","\n","    model = TrainModelForMultiClass(algo, 'xlnet-base-cased',train_df,3,args)\n","    result, model_outputs, wrong_predictions = EvalFromModel(model,eval_df)\n","\n","    with open('results/learning rate/'+str(j)+'epoch/'+algo+'-'+learn_rate+'-mcc.csv', 'a', newline='') as f:\n","      writer = csv.writer(f)\n","      writer.writerow([result['mcc'],result[\"acc\"],result[\"eval_loss\"]])\n","\n","    getMetrics(result[\"matr\"],3,'learning rate/'+str(j)+'epoch/'+algo+'-'+learn_rate)"]}]}