{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"name":"StandardTestingRun_Sentiment_Hyper.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"r90PIB5b7XEi"},"outputs":[],"source":["!pip install transformers\n","!pip install seqeval\n","!pip install tensorboardx\n","!pip install torchvision==0.4.0 -f https://download.pytorch.org/whl/cu100/torch_stable.html --no-cache-dir\n","!rm -rf .git\n","!rm -rf apex\n","!git clone https://github.com/NVIDIA/apex\n","!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n","!pip3 install simpletransformers\n","!pip install Afinn"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"colab_type":"code","id":"90TsvpNc0GY6","outputId":"a1faab2f-b91b-4cf7-e704-7741bfd9e4c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"nf7qSGtt7pw-"},"outputs":[],"source":["cd /content/drive/My\\ Drive/Colab\\ Notebooks"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"OKs0E_mczgjr"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import gc\n","import csv\n","\n","from CsvReader import ReadCsv\n","from HelperFunctions import ShuffleData, replace_all, ReplaceSentimentsWithIndexes, SplitArticlesIntoSentiments, getMetrics, TransformDataIntoDataframe, CalculateWeights\n","from TrainEvalModel import TrainModelForMultiClass, EvalFromModel\n","\n","data, categories, tendencies = ReadCsv('/content/drive/My Drive/Colab Notebooks Pascal/bert-etiki/etiki-data','test.csv','companies.csv', 'categories.csv','references.csv','tendencies.csv', 'topics.csv')\n","sentimentData = ReplaceSentimentsWithIndexes(data[:,[0,7,13]])\n","labels = np.unique(sentimentData[:,1])\n","weights = CalculateWeights(labels,sentimentData)\n","\n","for i in range(1,5):\n","  for j in range(2,6):\n","    positiveArticles, negativeArticles, controversialArticles = SplitArticlesIntoSentiments(sentimentData)\n","    positiveArticles = ShuffleData(positiveArticles)\n","    negativeArticles = ShuffleData(negativeArticles)\n","    controversialArticles = ShuffleData(controversialArticles)\n","\n","    trainData = np.vstack((positiveArticles[:int(len(positiveArticles)*0.7)],controversialArticles[:int(len(controversialArticles)*0.7)],negativeArticles[:int(len(negativeArticles)*0.7)]))\n","    testData = np.vstack((positiveArticles[int(len(positiveArticles)*0.7):],controversialArticles[int(len(controversialArticles)*0.7):],negativeArticles[int(len(negativeArticles)*0.7):]))\n","    train_df = TransformDataIntoDataframe(trainData)\n","    eval_df = TransformDataIntoDataframe(testData)\n","#------------------------------XLNet------------------------------\n","    algo = 'xlnet'        \n","    args = {'reprocess_input_data': True,\n","              'overwrite_output_dir': True,\n","              'num_train_epochs':j ,\n","              'silent':True,\n","              \n","              }\n","\n","    model = TrainModelForMultiClass(algo, 'xlnet-base-cased',train_df,3,args)\n","    result, model_outputs, wrong_predictions = EvalFromModel(model,eval_df)\n","\n","    with open('results/gradient/'+str(j)+'epoch/'+algo+'-mcc.csv', 'a', newline='') as f:\n","      writer = csv.writer(f)\n","      writer.writerow([result['mcc'],result[\"acc\"],result[\"eval_loss\"]])\n","\n","    getMetrics(result[\"matr\"],3,'gradient/'+str(j)+'epoch/'+algo)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"851E_z70-am0"},"source":["  args = {'reprocess_input_data': True,\n","            'overwrite_output_dir': True,\n","            'num_train_epochs':4 ,\n","            'silent':True\n","            }\n","\n","  model = TrainModelForMultiClass(algo, 'roberta-base',train_df,3,args,[1,1,1])\n","  result, model_outputs, wrong_predictions = EvalFromModel(model,eval_df)\n","\n","  with open('results/weight/'+algo+'-noweight-mcc-'+'.csv', 'a', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerow([result['mcc'],result[\"acc\"],result[\"eval_loss\"]])\n","\n","  getMetrics(result[\"matr\"],3,'weight/'+algo+'-noweight')\n","  torch.cuda.empty_cache()\n","  del model\n","  del result\n","  del model_outputs\n","  del wrong_predictions\n","  torch.cuda.empty_cache()\n","  gc.collect()"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"gliNzTeU0Fsh"},"outputs":[],"source":[""]}]}